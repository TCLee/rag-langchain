{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18dbb1fd-fc08-4aa7-a4f9-47d3529c550c",
   "metadata": {},
   "source": [
    "# Build a Retrieval Augmented Generation (RAG) App\n",
    "\n",
    "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\n",
    "\n",
    "This notebook will show how to build a simple Q&A application\n",
    "over a text data source. Along the way we’ll go over a typical Q&A architecture.\n",
    "\n",
    "> The code in this notebook is adapted from the LangChain tutorial [Build a RAG App](https://python.langchain.com/v0.2/docs/tutorials/rag/)\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "RAG is a technique for augmenting LLM knowledge with additional data.\n",
    "\n",
    "LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).\n",
    "\n",
    "LangChain has a number of components designed to help build Q&A applications, and RAG applications more generally. \n",
    "\n",
    "\n",
    "## Concepts\n",
    "\n",
    "A typical RAG application has two main components:\n",
    "\n",
    "**Indexing**: a pipeline for ingesting data from a source and indexing it. *This usually happens offline.*\n",
    "\n",
    "**Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n",
    "The most common full sequence from raw data to answer looks like:\n",
    "\n",
    "### Indexing\n",
    "1. **Load**: First we need to load our data. This is done with [DocumentLoaders](https://python.langchain.com/v0.2/docs/concepts/#document-loaders).\n",
    "2. **Split**: [Text splitters](https://python.langchain.com/v0.2/docs/concepts/#text-splitters) break large `Documents` into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "3. **Store**: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a [VectorStore](https://python.langchain.com/v0.2/docs/concepts/#vectorstores) and [Embeddings](https://python.langchain.com/v0.2/docs/concepts/#embedding-models) model.\n",
    "\n",
    "![index_diagram](img/rag_indexing.png)\n",
    "\n",
    "> Image from [LangChain Docs: Build a Retrieval Augmented Generation (RAG) App](https://python.langchain.com/v0.2/docs/tutorials/rag/#indexing)\n",
    "\n",
    "### Retrieval and generation\n",
    "\n",
    "4. **Retrieve**: Given a user input, relevant chunks are retrieved from storage using a [Retriever](https://python.langchain.com/v0.2/docs/concepts/#retrievers).\n",
    "5. **Generate**: A [ChatModel](https://python.langchain.com/v0.2/docs/concepts/#chat-models) (or LLM) produces an answer using a prompt that includes the question and the retrieved data\n",
    "\n",
    "![retrieval_diagram](img/rag_retrieval_generation.png)\n",
    "\n",
    "> Image from [LangChain Docs: Build a Retrieval Augmented Generation (RAG) App](https://python.langchain.com/v0.2/docs/tutorials/rag/#retrieval-and-generation)\n",
    "\n",
    "\n",
    "## Preview\n",
    "\n",
    "In this guide we’ll build a QA app over as website. The specific website we will use is the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post\n",
    "by Lilian Weng, which allows us to ask questions about the contents of\n",
    "the post.\n",
    "\n",
    "First, we need to choose a LLM to use. We will be using Google's **Gemini 1.5 Flash model** in this notebook because it is fast and it offers a free tier for us to play around with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72bd040b-e394-4c04-bda6-ffe713fc949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072372bf-9a9a-4f0f-ba45-fefe34a8e341",
   "metadata": {},
   "source": [
    "## 1. Indexing: Load\n",
    "\n",
    "We need to first load the blog post contents. We can use [DocumentLoaders](https://python.langchain.com/v0.2/docs/concepts/#document-loaders) for this, which are objects that load in data from a source and return a list of [Documents](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html).\n",
    "A `Document` is an object with some `page_content` (str) and `metadata` (dict).\n",
    "\n",
    "In this case we’ll use the\n",
    "[WebBaseLoader](https://python.langchain.com/v0.2/docs/integrations/document_loaders/web_base/), which uses `urllib` to load HTML from web URLs and `BeautifulSoup` to parse it to text. We can customize the HTML $\\longrightarrow$ text parsing by passing\n",
    "in parameters to the `BeautifulSoup` parser via `bs_kwargs` (see\n",
    "[BeautifulSoup\n",
    "docs](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup)).\n",
    "In this case only HTML tags with class `post-content`, `post-title`, or `post-header` are relevant, so we’ll remove all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8313db-d79a-4d54-b2c3-4a6681359722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43131"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(\n",
    "    class_=(\"post-title\", \n",
    "            \"post-header\", \n",
    "            \"post-content\")\n",
    ")\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\n",
    "        \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    ),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc981985-bebf-403a-a15c-8870a94423d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db893b0f-c13b-418c-a8fd-9ae0350847e4",
   "metadata": {},
   "source": [
    "### Go deeper\n",
    "\n",
    "`DocumentLoader`: Object that loads data from a source as list of `Documents`.\n",
    "\n",
    "- [Docs](https://python.langchain.com/v0.2/docs/how_to/#document-loaders):\n",
    "  Detailed documentation on how to use `DocumentLoaders`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6593e7-2aac-4bbb-bd99-b4ac26f9938b",
   "metadata": {},
   "source": [
    "## 2. Indexing: Split\n",
    "\n",
    "Our loaded document is over 42k characters long. \n",
    "This is too long to fit in the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
    "\n",
    "To handle this we’ll split the `Document` into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time.\n",
    "\n",
    "In this case we’ll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the\n",
    "[RecursiveCharacterTextSplitter](https://python.langchain.com/v0.2/docs/how_to/recursive_text_splitter/), which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n",
    "\n",
    "We set `add_start_index=True` so that the character index at which each split Document starts within the initial Document is preserved as metadata attribute “start_index”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "653e04f2-1f71-4837-a394-d89dedf4bb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200, \n",
    "    add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e19761bd-7672-466d-ba9e-2c40bb9bcec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "969"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f88f121c-7298-4614-925b-802ca49b61f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'start_index': 7056}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[10].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff613eae-b2f8-416a-a124-5fcb6ed0092c",
   "metadata": {},
   "source": [
    "### Go deeper\n",
    "\n",
    "`TextSplitter`: Object that splits a list of `Document`s into smaller chunks. Subclass of `DocumentTransformer`s.\n",
    "\n",
    "- Learn more about splitting text using different methods by reading the [how-to docs](https://python.langchain.com/v0.2/docs/how_to/#text-splitters)\n",
    "\n",
    "\n",
    "## 3. Indexing: Store\n",
    "\n",
    "Now we need to index our text chunks so that we can search over them\n",
    "later. The most common way to do this is to embed the contents of\n",
    "each document split and insert these embeddings into a vector database\n",
    "such as Chroma. When we want to search over our splits, we take a\n",
    "text search query, embed it, and perform some sort of “similarity”\n",
    "search to identify the stored splits with the most similar embeddings to\n",
    "our query embedding. The simplest similarity measure is cosine\n",
    "similarity — we measure the cosine of the angle between each pair of\n",
    "embeddings (which are very high dimensional vectors).\n",
    "\n",
    "We can embed and store all of our document splits in a single command\n",
    "using the \n",
    "[Chroma](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/)\n",
    "vector store and \n",
    "[GoogleGenerativeAIEmbeddings](https://python.langchain.com/v0.2/docs/integrations/text_embedding/google_generative_ai/) \n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ae23d3a-51b3-4d22-92ff-1d02ca6310fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/text-embedding-004\"\n",
    ")\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a957dba-7799-4b07-aeb5-337f109ea3de",
   "metadata": {},
   "source": [
    "### Go deeper\n",
    "\n",
    "`Embeddings`: Wrapper around a text embedding model, used for converting\n",
    "text to embeddings.\n",
    "\n",
    "- [Docs](https://python.langchain.com/v0.2/docs/how_to/embed_text/): Detailed documentation on how to use embeddings.\n",
    "\n",
    "`VectorStore`: Wrapper around a vector database, used for storing and\n",
    "querying embeddings.\n",
    "\n",
    "- [Docs](https://python.langchain.com/v0.2/docs/how_to/vectorstores/): Detailed documentation on how to use vector stores.\n",
    "\n",
    "\n",
    "This completes the **Indexing** portion of the pipeline. At this point\n",
    "we have a query-able vector store containing the chunked contents of our\n",
    "blog post. Given a user question, we should ideally be able to return\n",
    "the snippets of the blog post that answer the question.\n",
    "\n",
    "\n",
    "## 4. Retrieval and Generation: Retrieve\n",
    "\n",
    "Now let’s write the actual application logic. We want to create a simple\n",
    "application that takes a user question, searches for documents relevant\n",
    "to that question, passes the retrieved documents and initial question to\n",
    "the LLM, and returns an answer.\n",
    "\n",
    "First we need to define our logic for searching over documents.\n",
    "LangChain defines a\n",
    "[Retriever](https://python.langchain.com/v0.2/docs/concepts/#retrievers/) \n",
    "interface, which wraps an index that can return relevant `Documents` \n",
    "given a string query.\n",
    "\n",
    "The most common type of `Retriever` is the\n",
    "[VectorStoreRetriever](https://python.langchain.com/v0.2/docs/how_to/vectorstore_retriever/),\n",
    "which uses the similarity search capabilities of a vector store to\n",
    "facilitate retrieval. Any `VectorStore` can easily be turned into a\n",
    "`Retriever` with `VectorStore.as_retriever()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1ce7762-9316-4e7f-92ed-3f0525d5a0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\", \n",
    "    search_kwargs={\"k\": 6} # return top 6 most relevant documents\n",
    ")\n",
    "\n",
    "retrieved_docs = retriever.invoke(\n",
    "    \"What are the approaches to Task Decomposition?\"\n",
    ")\n",
    "\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "523862fc-6aed-4228-968b-4458f2db99f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423701da-d017-4169-8969-a8f30c9190ec",
   "metadata": {},
   "source": [
    "### Go deeper\n",
    "\n",
    "Vector stores are commonly used for retrieval, but there are other ways\n",
    "to do retrieval, too.\n",
    "\n",
    "`Retriever`: An object that returns `Document`s given a text query\n",
    "\n",
    "- [Docs](https://python.langchain.com/v0.2/docs/how_to/#retrievers): Further\n",
    "  documentation on the interface and built-in retrieval techniques.\n",
    "\n",
    "\n",
    "## 5. Retrieval and Generation: Generate\n",
    "\n",
    "Let’s put it all together into a chain that takes a question, retrieves\n",
    "relevant documents, constructs a prompt, passes that to a LLM, and\n",
    "parses the output.\n",
    "\n",
    "We’ll use a prompt for RAG that is checked into the LangChain prompt hub\n",
    "([here](https://smith.langchain.com/hub/rlm/rag-prompt))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b902d00-a2cd-492f-8048-8d390956b5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "\n",
    "example_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46b030ef-d24a-4ebe-90ca-e88452852552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filler context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd75273-3bad-46fd-bd5d-5998fb89c51a",
   "metadata": {},
   "source": [
    "We’ll use the \n",
    "[LCEL Runnable](https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel)\n",
    "protocol to define the chain, allowing us to \n",
    "\n",
    "- pipe together components and functions in a transparent way \n",
    "- automatically trace our chain in LangSmith \n",
    "- get streaming, async, and batched calling out of the box.\n",
    "\n",
    "Here is the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "423a590b-63c6-4d96-9561-9ec6791c54ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This can be done through various methods, including prompting an LLM with instructions, using task-specific instructions, or with human input. By breaking down tasks, it becomes easier for models to understand and solve them. \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs, \n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917ba12d-392b-4938-988a-c852fd19b279",
   "metadata": {},
   "source": [
    "Let's dissect the LCEL to understand what's going on.\n",
    "\n",
    "First: each of these components (`retriever`, `prompt`, `llm`, etc.) are instances of \n",
    "[Runnable](https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel)\n",
    ". This means that they implement the same methods -- such as sync and async `.invoke`, `.stream`, or `.batch` -- which makes them easier to connect together. They can be connected into a [RunnableSequence](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableSequence.html) -- another Runnable -- via the `|` (pipe) operator.\n",
    "\n",
    "LangChain will automatically cast certain objects to runnables when met with the `|` operator. Here, `format_docs` is cast to a \n",
    "[RunnableLambda](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableLambda.html)\n",
    ", and the dict with `\"context\"` and `\"question\"` is cast to a \n",
    "[RunnableParallel](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableParallel.html)\n",
    ". The details are less important than the bigger point, which is that each object is a Runnable.\n",
    "\n",
    "Let's trace how the input question flows through the above runnables.\n",
    "\n",
    "As we've seen above, the input to `prompt` is expected to be a dict with keys `\"context\"` and `\"question\"`. So the first element of this chain builds runnables that will calculate both of these from the input question:\n",
    "- `retriever | format_docs` passes the question through the retriever, generating\n",
    "[Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
    " objects, and then to `format_docs` to generate strings;\n",
    "- `RunnablePassthrough()` passes through the input question unchanged.\n",
    "\n",
    "That is, if you constructed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c68e70d7-de4e-4990-a13b-63f92fc947ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs, \n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    ")\n",
    "\n",
    "formatted_prompt = chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d33d16-c1ad-42e3-a1da-d5787f89ad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_prompt.messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5db70c4-4f39-42e2-ba92-bd990c1b3e90",
   "metadata": {},
   "source": [
    "Then `chain.invoke(question)` would build a formatted prompt, ready for inference. (Note: when developing with LCEL, it can be practical to test with sub-chains like this.)\n",
    "\n",
    "The last steps of the chain are `llm`, which runs the inference, and `StrOutputParser()`, which just plucks the string content out of the LLM's output message.\n",
    "\n",
    "\n",
    "### Built-in chains\n",
    "\n",
    "If preferred, LangChain includes convenience functions that implement the above LCEL. We compose two functions:\n",
    "\n",
    "- [create_stuff_documents_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html)\n",
    "specifies how retrieved context is fed into a prompt and LLM. In this case, we will \"stuff\" the contents into the prompt -- i.e., we will include all retrieved context without any summarization or other processing. It largely implements our above `rag_chain`, with input keys `context` and `input`-- it generates an answer using retrieved context and query.\n",
    "\n",
    "- [create_retrieval_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval.create_retrieval_chain.html)\n",
    "adds the retrieval step and propagates the retrieved context through the chain, providing it alongside the final answer. It has input key `input`, and includes `input`, `context`, and `answer` in its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9359553-fda8-4a12-bdee-ea1fdfb09db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Decomposition is a technique used to break down complex tasks into smaller, simpler steps. This is often done by using chain of thought (CoT) prompting, where the model is instructed to \"think step by step.\" This helps the model utilize more computation at test time and allows for better interpretation of its thinking process. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffef34e-2129-4944-8f4e-71d10948d93c",
   "metadata": {},
   "source": [
    "#### Returning sources\n",
    "Often in Q&A applications it's important to show users the sources that were used to generate the answer. LangChain's built-in `create_retrieval_chain` will propagate retrieved source documents through to the output in the `\"context\"` key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e6cad13-4789-460b-8ee1-bc0874d7d3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}\n",
      "\n",
      "page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}\n",
      "\n",
      "page_content='Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 30952}\n",
      "\n",
      "page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\" metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}\n",
      "\n",
      "page_content='Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17414}\n",
      "\n",
      "page_content='(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 18661}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for document in response[\"context\"]:\n",
    "    print(document)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b42e53-be71-4952-87f8-f6467db54199",
   "metadata": {},
   "source": [
    "#### Customizing the prompt\n",
    "\n",
    "As shown above, we can load prompts (e.g., \n",
    "[this RAG prompt](https://smith.langchain.com/hub/rlm/rag-prompt)\n",
    ") from the prompt hub. The prompt can also be easily customized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fa6131f-0765-4bcf-b674-e7d31889ceb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This can be done by using prompting techniques like Chain of Thought (CoT) or Tree of Thoughts (ToT), which guide the model to think step by step and explore multiple reasoning possibilities. Task decomposition can also be achieved through task-specific instructions or human input. \\nThanks for asking! \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "custom_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "custom_rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b3d289-be3c-4160-b531-e98695a09250",
   "metadata": {},
   "source": [
    "# Conversational RAG\n",
    "\n",
    "In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.\n",
    "\n",
    "In this guide we focus on **adding logic for incorporating historical messages.** Further details on chat history management is \n",
    "[covered here](https://python.langchain.com/v0.2/docs/how_to/message_history/).\n",
    "\n",
    "> The following section of this notebook is adapted from the LangChain tutorial \n",
    "[Conversational RAG](https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/).\n",
    "\n",
    "\n",
    "### Adding chat history\n",
    "\n",
    "The chain we have built uses the input query directly to retrieve relevant context. But in a conversational setting, the user query might require conversational context to be understood. For example, consider this exchange:\n",
    "\n",
    "> Human: \"What is Task Decomposition?\"\n",
    ">\n",
    "> AI: \"Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable for an agent or model.\"\n",
    ">\n",
    "> Human: \"What are common ways of doing it?\"\n",
    "\n",
    "In order to answer the second question, our system needs to understand that \"it\" refers to \"Task Decomposition.\"\n",
    "\n",
    "We'll need to update two things about our existing app:\n",
    "\n",
    "1. **Prompt**: Update our prompt to support historical messages as an input.\n",
    "2. **Contextualizing questions**: Add a sub-chain that takes the latest user question and reformulates it in the context of the chat history. This can be thought of simply as building a new *\"history aware\"* retriever. Whereas before we had:\n",
    "   - `query` $\\longrightarrow$ `retriever`  \n",
    "     Now we will have:\n",
    "   - `(query, conversation history)` $\\longrightarrow$ `LLM` $\\longrightarrow$ `rephrased query` $\\longrightarrow$ `retriever`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5879b0e0-d0b5-4a15-b21a-67387abc9d3c",
   "metadata": {},
   "source": [
    "#### Contextualizing the question\n",
    "\n",
    "First we'll need to define a sub-chain that takes historical messages and the latest user question, and reformulates the question if it makes reference to any information in the historical information.\n",
    "\n",
    "We'll use a prompt that includes a `MessagesPlaceholder` variable under the name \"chat_history\". This allows us to pass in a list of Messages to the prompt using the \"chat_history\" input key, and these messages will be inserted after the system message and before the human message containing the latest question.\n",
    "\n",
    "Note that we leverage a helper function [create_history_aware_retriever](https://api.python.langchain.com/en/latest/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html) for this step, which manages the case where `chat_history` is empty, and otherwise applies `prompt | llm | StrOutputParser() | retriever` in sequence.\n",
    "\n",
    "`create_history_aware_retriever` constructs a chain that accepts keys `input` and `chat_history` as input, and has the same output schema as a retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebfee6e5-da08-4b09-85b1-22309de67bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808332e3-b6f4-4ecc-bdb2-55ddd0c41f29",
   "metadata": {},
   "source": [
    "This chain prepends a rephrasing of the input query to our retriever, so that the retrieval incorporates the context of the conversation.\n",
    "\n",
    "Now we can build our full QA chain. This is as simple as updating the retriever to be our new `history_aware_retriever`.\n",
    "\n",
    "Again, we will use [create_stuff_documents_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html) to generate a `question_answer_chain`, with input keys `context`, `chat_history`, and `input`-- it accepts the retrieved context alongside the conversation history and query to generate an answer.\n",
    "\n",
    "We build our final `rag_chain` with [create_retrieval_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval.create_retrieval_chain.html). This chain applies the `history_aware_retriever` and `question_answer_chain` in sequence, retaining intermediate outputs such as the retrieved context for convenience. It has input keys `input` and `chat_history`, and includes `input`, `chat_history`, `context`, and `answer` in its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4db00d1-46c2-487e-863f-72a16facf93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20893c4a-0a14-455c-9568-97bc8211a073",
   "metadata": {},
   "source": [
    "Let's try this. Below we ask a question and a follow-up question that requires contextualization to return a sensible response. Because our chain includes a `\"chat_history\"` input, the caller needs to manage the chat history. We can achieve this by appending input and output messages to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14c336a5-635a-46e2-9e19-7937ee11a13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition can be achieved in several ways:\n",
      "\n",
      "* **LLM Prompting:**  Simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" can guide an LLM to break down a task.\n",
      "* **Task-Specific Instructions:** Using instructions tailored to the task, such as \"Write a story outline\" for writing a novel, can help decompose the task into manageable steps.\n",
      "* **Human Input:**  Humans can directly provide a breakdown of the task into sub-tasks, especially for tasks that require domain expertise. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24edccc1-b99c-4b97-ad66-4e6b9e073144",
   "metadata": {},
   "source": [
    "#### Stateful management of chat history\n",
    "\n",
    "Here we've gone over how to add application logic for incorporating historical outputs, but we're still manually updating the chat history and inserting it into each input. In a real Q&A application we'll want some way of persisting chat history and some way of automatically inserting and updating it.\n",
    "\n",
    "For this we can use:\n",
    "\n",
    "- [BaseChatMessageHistory](https://api.python.langchain.com/en/latest/langchain_api_reference.html#module-langchain.memory): Store chat history.\n",
    "- [RunnableWithMessageHistory](https://python.langchain.com/v0.2/docs/how_to/message_history/): Wrapper for an LCEL chain and a `BaseChatMessageHistory` that handles injecting chat history into inputs and updating it after each invocation.\n",
    "\n",
    "For a detailed walkthrough of how to use these classes together to create a stateful conversational chain, head to the [How to add message history (memory)](https://python.langchain.com/v0.2/docs/how_to/message_history) LCEL page.\n",
    "\n",
    "Below, we implement a simple example of the second option, in which chat histories are stored in a simple dict. LangChain manages memory integrations with [Redis](https://python.langchain.com/v0.2/docs/integrations/memory/redis_chat_message_history/) and other technologies to provide for more robust persistence.\n",
    "\n",
    "Instances of `RunnableWithMessageHistory` manage the chat history for you. They accept a config with a key (`\"session_id\"` by default) that specifies what conversation history to fetch and prepend to the input, and append the output to the same conversation history. Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec494da1-fd54-4e12-a119-a47accd0340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec58d946-46b7-4b77-808c-671293140362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run d7c382a3-6a64-46c9-9c37-f29ad4eed85f not found for run 330f852a-a611-48d2-8744-2f2b665df735. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a technique used to break down complex tasks into smaller, more manageable steps. This process is often used in conjunction with chain-of-thought (CoT) prompting, which encourages the model to \"think step by step\" and utilize more computation to solve complex problems. By decomposing tasks, it becomes easier for a model to understand and solve them. \\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65202ab3-5973-45c1-a24d-750af0736f76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 7183abb1-0fc9-4d58-a9fb-3f87c3d5f295 not found for run 7bf4769d-a64c-4efe-9c6f-d8fef4cf085c. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task decomposition can be achieved in a few common ways:\\n\\n1. **LLM prompting:** Simple prompts like \"Steps for XYZ.\\\\n1.\" or \"What are the subgoals for achieving XYZ?\" can guide the LLM to break down the task.\\n2. **Task-specific instructions:** Providing instructions tailored to the task, such as \"Write a story outline\" for writing a novel, can help the LLM understand the necessary steps.\\n3. **Human input:** Humans can directly provide the task decomposition, either by outlining the steps themselves or by providing examples of how the task can be broken down. \\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26448ed0-de00-45af-a5da-80b30c7a13be",
   "metadata": {},
   "source": [
    "The conversation history can be inspected in the `store` dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b72e70bb-f3e5-49a3-99d5-e9e5caaecb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is Task Decomposition?\n",
      "\n",
      "AI: Task decomposition is a technique used to break down complex tasks into smaller, more manageable steps. This process is often used in conjunction with chain-of-thought (CoT) prompting, which encourages the model to \"think step by step\" and utilize more computation to solve complex problems. By decomposing tasks, it becomes easier for a model to understand and solve them. \n",
      "\n",
      "\n",
      "User: What are common ways of doing it?\n",
      "\n",
      "AI: Task decomposition can be achieved in a few common ways:\n",
      "\n",
      "1. **LLM prompting:** Simple prompts like \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\" can guide the LLM to break down the task.\n",
      "2. **Task-specific instructions:** Providing instructions tailored to the task, such as \"Write a story outline\" for writing a novel, can help the LLM understand the necessary steps.\n",
      "3. **Human input:** Humans can directly provide the task decomposition, either by outlining the steps themselves or by providing examples of how the task can be broken down. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in store[\"abc123\"].messages:\n",
    "    if isinstance(message, AIMessage):\n",
    "        prefix = \"AI\"\n",
    "    else:\n",
    "        prefix = \"User\"\n",
    "\n",
    "    print(f\"{prefix}: {message.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118fba7e-6bf4-4984-97f9-519272ad0073",
   "metadata": {},
   "source": [
    "### Tying it together\n",
    "\n",
    "![retrieval chain with chat history](img/conversational_retrieval_chain.png)\n",
    "\n",
    "> Image from [LangChain Docs: Conversational RAG](https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/#tying-it-together)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdae155-d2e2-402c-9448-9f43528ec07e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've covered the steps to build a basic Q&A app over data:\n",
    "\n",
    "- Loading data with a\n",
    "[Document Loader](https://python.langchain.com/v0.2/docs/concepts/#document-loaders)\n",
    "- Chunking the indexed data with a\n",
    "[Text Splitter](https://python.langchain.com/v0.2/docs/concepts/#text-splitters)\n",
    "to make it more easily usable by a model\n",
    "- [Embedding the data](https://python.langchain.com/v0.2/docs/concepts/#embedding-models)\n",
    "and storing the data in a\n",
    "[vectorstore](https://python.langchain.com/v0.2/docs/how_to/vectorstores/)\n",
    "- [Retrieving](https://python.langchain.com/v0.2/docs/concepts/#retrievers)\n",
    "the previously stored chunks in response to incoming questions\n",
    "- Generating an answer using the retrieved chunks as context\n",
    "- Added chat history to support asking follow up questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd6d156-491f-4b4c-9834-803872c98cd2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
